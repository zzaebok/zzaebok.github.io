---
title: "기계 번역 평가를 위한 COMET 스코어"
date: 2024-05-27 21:56:28 -0400
categories: machine_learning nlp
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "left"
});
</script>

## Lexical Metric의 한계

기계 번역(Machine Translation) 모델의 성능을 평가하는 Metric 에는 여러 가지가 있지만, 가장 간단하면서도 대표적인 것은 `BLEU` 라는 지표이다.
`BLEU` 스코어는 Reference (정답문) 과 Hypothesis (번역 결과) 의 유사성을 n-gram을 이용하여 측정하는 방법이다.
쉽게 말해서 번역 모델이 생성한 output (hypothesis) 와 정답 문장의 n-gram 이 얼마나 '많이' 겹쳐있냐를 판단하는 아주 직관적인 Metric이다.
이 글은 BLEU 자체를 다루는 포스트가 아니므로, 정확한 BLEU 계산을 위해서는 [이 곳](https://wikidocs.net/31695)을 참조하길 바란다.

하지만 이런 어휘적 방법에는 아주 큰 문제가 존재한다.
바로, 표현적으로는 다르지만 "의미적으로" 유사한 것을 측정할 수 없다는 것이다.
아래 문장에 대한 두 가지 번역으로 살펴보자.

> Source: "The weather is nice today"
> Reference: "오늘은 날씨가 좋다"
>- Translation1. "오늘 날씨가 좋다"
>- Translation2. "오늘 날씨가 맑다"

두 가지 문장이 모두 원문을 잘 해석하고 있는 한국어 번역문이다.
하지만 이런 문장을 `BLEU`로 측정한다면, Translation2 의 경우 동일한 "의미"임에도 상대적으로 낮은 기계번역 품질이라고 평가절하 당할 것이다.

## COMET 스코어

이렇게 "의미적인" 평가가 `BLEU`로 불가능해지자 등장한 Metric이 바로 COMET이다.
COMET은 pre-trained cross-lingual 모델인 XLM-RoBERTa 와 같은 모델들의 Encoder를 활용하여 문장에 대한 Score를 측정하게 된다.

먼저 데이터셋을 준비해야하는데, 당연하게도 입력 문장 (Source), 번역문 (Hypothesis), 그리고 정답 문장 (Reference) 와, 번역문에 대한 Score 데이터가 필요하다.
저자들은 이 데이터 종류에 따라 `Estimator` 모델과 `Ranking` 모델을 만들었다.
`Estimator`모델의 경우 학습 과정 자체가 (입력, 번역, 정답)을 Input으로 하여, Output Score를 직접 예측하는 Regression 모델이며,
`Ranking`모델의 경우 학습 때 (입력, 좋은 번역, 나쁜 번역, 정답)을 Triplet Loss로 학습한 뒤, 실제 Inference 시에는 (입력, 번역, 정답) 사이의 임베딩 Distance를 이용하는 모델이다.

## Estimator 버전

<img src="https://imgur.com/Fm3Yrz5.png" width="600">

모델에서 Pretrained Encoder는 Cross-lingual model의 Encoder 이며, 논문에서는 대표적으로 XLM-R 모델을 사용했다.
여기서 개인적으로 그림이 잘못되었다고 생각하는 것은 바로 3가지 Input이 "각각" Encoder에 들어간다는 것이다. (마치 그림은 3가지 Input을 Concat한 뒤에 Encoder에 넣는 것처럼 되어있다.)

각각 3가지 문장이 Encoder를 통과한 뒤, Pooling Layer에서 이 3가지 문장에 대한 Embedding을 생성하게 된다.
Pooling 방법은, 각 층의 Word Embedding을 가중평균한 뒤, 문장 내 각 Word Embedding을 평균하는 것으로 이루어져 있다.
이후 3가지 Embedding을 Concatenation 하여 Feed-Forward를 통과할 대표 Embedding을 만들어주게 된다.

$$ x = [h;r;h\odot s;h\odot r;|h-s|;|h-r|] $$

그리고 이 output embedding을 Feed-Forward 에 붙여서 학습하면 Estimator 모델을 만들 수 있다.

## Ranking 버전

<img src="https://imgur.com/HjfjUQN.png" width="600">

Ranking 모델도 뭔가 그림이 잘못되었다고 생각하는데, Sentence Embedding은 각각의 문장으로부터 개별 도출된다. (S, H+, H-, R)
각 4개의 문장이 Encoder, Pooling Layer를 거치며 Embedding으로 변환되고, 이 Embedding들을 아래처럼 Triplet Margin Loss를 학습시킨다.

<img src="https://imgur.com/L3VHCxQ.png" width="400">

단, 이렇게 학습을 진행하게 되면 Metric으로써 어떻게 작동할 수 있을까?
바로 아래 처럼, Inference 시에는 Embedding 사이의 유클리드 거리를 이용하여 (번역문, 입력문), (번역문, 정답문) 사이의 조화평균을 이용하여 Score를 0에서 1에 Bound시키게 된다.

<img src="https://imgur.com/ENtu6qM.png" width="400">

## 실험 결과

저자들은 HTER, DA, MQM 방식으로 구축된 (입력, 번역, 정답, 점수) 데이터셋을 이용하여 두 가지 모델을 모두 학습시켰다.

Kendall's Tau correlation (얼마나 Human Judgement와 비슷한지 비율로 평가) 으로 평가한 모델의 성능은 아래와 같다. (HTER, MQM은 Estimator 모델이고 Ranking은 Ranking 모델이다.)

<img src="https://imgur.com/iV2JUVp.png" width="500">

뿐만 아니라, English Centric 데이터로 학습했음에도 불구하고 English Pair가 아닌 Language Direction에 대해서도 좋은 성능을 보여준다.

<img src="https://imgur.com/hZnnzKH.png" width="400">

COMET은 이처럼 기계번역 분야에서 "의미적인" 유사성을 평가하기 위한 중요한 Metric으로 사용되고 있는 지표이다.
`BLEU`처럼 쉽고 간편하며 모든 Language Direction에 확장성이 있는 것은 아니지만 조금 더 정교하고 섬세한 지표임에는 틀림없다.
이 뒤로 번역문을 MQM 방식처럼 문제가 있는 부분 (error span)을 찾아내어 Explainable하게 설명하는 XCOMET 과 같은 모델들도 등장하였으니 관심있는 분들은 찾아봐도 좋을 것 같다.

## References
- COMET: https://arxiv.org/pdf/2009.09025
- XCOMET: https://arxiv.org/pdf/2310.10482