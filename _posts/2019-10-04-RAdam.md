---
title: "RAdam 정리"
date: 2019-10-04 15:48:28 -0400
categories: machine_learning
permalink: /deep_learning/RAdam/
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: "left"
});
</script>

## Intro ##
딥러닝을 이용한 학습 시에 가중치를 업데이트하기 위해 꼭 필요한 Opimizer, 그 중에 가장 많이 사용되는 것은 누가 뭐라고 해도 Adam optimizer일 것입니다. Adam optimizer는 adaptive learning rate를 기반으로 하는 특징을 가졌는데, 이 adaptive learning rate를 식으로 표현하면 다음과 같습니다.

$$ v_t = \beta_2 v_{t-1} + (1-\beta_2)(\nabla_\theta J(\theta))^2$$

$$ \hat{v_t} = \frac{v_t}{1-\beta_2^t} $$

$$ \theta = \theta - \frac{\eta}{\sqrt{\hat{v_t}+\epsilon}}\hat{m_t} $$

즉, 지금까지의 상대적인 업데이트 양에 따라 step size를 조정해준다는 것입니다. 그런데 이 Adam optimizer에도 사실 문제가 있다고 주장하며 이 문제를 해결한 Adam의 변형이 Rectified Adam optimizer가 세상에 나왔습니다.
오늘은 이 Rectified Adam, 줄여서 RAdam이란 무엇인 지에 대해 포스팅하겠습니다.

## Adam optimizer의 한계점 ##
저처럼 단순히 'Adam을 쓰면 된다. 제일 낫다'라고만 들었던 사람들은 Adam이 가진 한계점이 무엇인지 잘 모를 수 있습니다.
사실 Adam의 한계점은 단순히 Adam에 한정되는 것이 아니라, Adaptiver learning rate을 사용하는 optimizer에서는 모두 발생하는 현상입니다.
이는 바로 'Bad local optima convergence problem'입니다. 이는 학습 초기에 샘플이 매우 부족하여 adaptive learning rate의 분산이 매우 커지고
이에 따라 최적이 아닌 local optima에 너무 일찍 도달하여 학습이 거의 일어나지 않는 현상입니다.
쉽게 생각해보면 처음 무언가를 배운다고 시작하면 보는 것마다 엄청 낯설어 혼란스러워 한다고 볼 수 있겠죠??

![gradients](https://i.imgur.com/7BoV1yq.png)

위 그림과 같이, Adam optimizer의 경우 학습 초기를 조금만 지나면 gradients의 값이 e^-20과 같이 매우매우매우 작은 값으로 변하는 것을 확인할 수 있습니다.
다른 말로 하면 초기에 어떠한 bad local optima에 수렴해버리기 때문에 그 이상 학습이 일어나지 않는 것임을 확인할 수 있습니다.

또한 수학적 증명으로서도 학습 초기 adaptive learning rate의 분산을 확인할 수 있습니다.

![variance](https://i.imgur.com/lNUF7PL.png)

gradient가 Normal distribution을 따른다는 것과, adaptive learning rate term이 Scaled inverse chi square distribution을 따른다는 것을 이용하면 위와 같은 식을 세울 수 있는데, 여기서 $$\rho$$는 자유도로서 학습 step이 진행된 정도라고 생각하시면 편합니다. 논문에서는 이렇게 생긴 분산이 자유도 $$\rho$$에 의해 미분하면 단조감소 형태가 됨으로써 ( 즉, $$\rho$$가 작을 수록 분산이 커지는 ) 학습 초기의 불완전성을 증명하였습니다. 

## Warmup heuristic ##
이러한 학습 초기 convergence problem을 해결하기 위해 다양한 시도들이 있었고, 지금까지 각광받는 방법은 바로 warmup heuristic이었습니다. 이는 말그대로
학습 초기에 warmup이 필요하다는 것입니다. 예를 들어 내가 정한 learning rate가 0.01이라고 한다면 처음 10 step 동안은 0.001, 0.002, 0.003 ~ 0.01까지 선형적으로 조금씩만
증가하는 learning rate을 사용하는 것입니다. 이는 반대로 말하면 샘플이 적은 초기에 아주아주 작은 learning rate을 사용함으로써 bad local optima
로의 학습이 일어나지 않게 만드는 것이었습니다. 하지만 이러한 방식은 warmup step을 따로 정해줘야하기 때문에 찾아야하는 hyperparmeter가 하나 더 생기는 큰 단점이 존재합니다. 또한, warmup heuristic이 어떻게 학습을 안정화 시킬 수 있는 가에 대한 이론적 증명이 결여되어 있었기 때문에
사용자들의 trial and error stage를 꼭 거쳐야 했던 불편함이 있었습니다.
저자는 Adam-2k와 Adam-eps라는 학습 초기 분산을 줄인 모델과 warmup heuristic이 결국은 같은 행동양식을 보인다는 것을 보임으로써, warmup heuristic의 역할이 결론적으로는 adaptive learning rate의 분산을 줄이는 것임을 증명하였습니다.


## RAdam ##
RAdam은 Rectified Adam의 준말로서 adaptive learning rate term의 분산을 rectify한다는 의미입니다. 즉 우리가 구한 분산 식을 거꾸로 이용해 분산을 consistent하게 만들 수 있는 rectification term을 구하고 이를 곱해줌으로써 학습의 안정성을 얻을 수 있습니다.

![rectification](https://i.imgur.com/RpkweeC.png)

![rectification2](https://i.imgur.com/bzmKX1t.png)

여기서 $$\rho_t$$는 $$\rho$$를 step size t를 이용하여 estimation한 값입니다. 자세한 수식은 [논문](https://arxiv.org/pdf/1908.03265.pdf)을 보시면 확인하실 수 있는데 결국은 scaled inverse chi square distribution와 지수평균의 단순평균 근사 특징을 이용하여 구한 값입니다. 

## Result ##
한 마디로 정리하자면 RAdam은 Adam의 수식에 rectification을 곱해줌으로써 학습 초기에 일어날 수 있는 bad local optima problem을 해결하고, 학습 안정성을 높였다고 할 수 있습니다.

![result](https://i.imgur.com/EMj05x3.png)

실제로 학습에 이용한 결과 초기 learning rate에 따라 학습에 큰 차이를 보이는 Adam이나 SGD와 달리 굉장히 안정적인 결과를 얻을 수 있습니다. 또한 warmup heuristic 방식처럼 비직관적이지 않고 자동으로 분산을 줄여주기 때문에 사용자 입장에서의 편리성을 크게 얻었다고 할 수 있습니다.
(저도 호기심에 예전에 했던 kaggle 프로젝트에 RAdam을 적용해보았더니 score가 꽤 많이 오르더라구요. 시간이 부족한분들은 learning rate를 일일이 찾기보다는 RAdam을 쓰시면 편할 거 같습니다.)

[RAdam 공식 깃헙](https://github.com/LiyuanLucasLiu/RAdam)을 참고하셔서 공부하시면 큰 도움이 될 것 같습니다.
